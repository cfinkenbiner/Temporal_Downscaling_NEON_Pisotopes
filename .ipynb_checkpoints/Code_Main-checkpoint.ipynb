{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Precipitation and Isotope NEON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rpy2 so R scripts can run within jupyter notebook - run this one time\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "# This rpy2 'Rmagic' command allows to run the entire cell block as R code\n",
    "\n",
    "# Install R packages - run this one time\n",
    "install.packages(\"neonUtilities\")\n",
    "install.packages('openxlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load required libraries - run this every time\n",
    "library(neonUtilities)\n",
    "library(openxlsx)\n",
    "\n",
    "# creating a list of site names: this will iterate through all sites, or can be adapted to only download sites of interest\n",
    "siteNames <- c(\"DELA\", \"LENO\", \"TALL\", \"BARR\", \"BONA\", \"HEAL\", \"TOOL\", \"SRER\", \"SJER\", \"CPER\", \"NIWO\", \"RMNP\",\n",
    "             \"STER\", \"OSBS\", \"JERC\", \"PUUM\", \"KONZ\", \"UKFS\", \"SERC\", \"HARV\", \"UNDE\", \"BART\", \"NOGP\", \"WOOD\", \"OAES\",\n",
    "              \"GUAN\", \"GRSM\", \"ORNL\", \"CLBJ\", \"MOAB\", \"ONAQ\", \"BLAN\", \"MLBS\", \"SCBI\", \"WREF\", \"STEI\", \"YELL\")\n",
    "\n",
    "for (val in siteNames) {\n",
    "    siteName <- val\n",
    "    \n",
    "    # download isotope data\n",
    "    IsoData <- loadByProduct(dpID=\"DP1.00038.001\", site=siteName)\n",
    "    list2env(IsoData, .GlobalEnv)\n",
    "    destination <- paste(\"IsoData/\", siteName, \"IsoData.xlsx\", sep=\"\") # moves into IsoData folder and creates file name\n",
    "    write.xlsx(wdi_isoPerSample, destination, row.names=F)\n",
    "    \n",
    "    # download precip data - primary data if available, secondary data if not\n",
    "    PrecipData <- loadByProduct(dpID=\"DP1.00006.001\", site=siteName)\n",
    "    list2env(PrecipData, .GlobalEnv)\n",
    "    destination <- paste(\"PrecipData/\", siteName, \"PrecipData.xlsx\", sep=\"\") # moves into PrecipData folder and creates file name\n",
    "    if (exists(\"PRIPRE_30min\")) {\n",
    "        write.xlsx(PRIPRE_30min, destination, row.names=F)\n",
    "    } else {\n",
    "        write.xlsx(SECPRE_30min, destination, row.names=F)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate a downscaled daily data product at each NEON site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Import Statements\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOTDIR = os.getcwd() # Home directory\n",
    "CODEDIR = ROOTDIR + '/CODE/' # Code directory\n",
    "DATADIR = ROOTDIR + '/DATA/' # Data directory\n",
    "\n",
    "os.chdir(CODEDIR) # Change directory\n",
    "import changeTimes\n",
    "import getRunningMean\n",
    "import conditional_copula_ts\n",
    "import iso_sine_signal\n",
    "import calcSiteStats\n",
    "\n",
    "os.chdir(ROOTDIR) # Change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined variables\n",
    "sitenames = [\"BARR\"]\n",
    "ensemble_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have length at least 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-584806f40764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0madj_18Ob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxday_Ob\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtsY_sine_wave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mcalcSiteStats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msitenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlamda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxday_Pb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madj_2Hb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madj_18Ob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m### the stats are labeled with 'B' here because they are of the stochastic component - not the original time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/NEON/Temporal_Downscaling_NEON_Pisotopes/CODE/calcSiteStats.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(site_stats, site, lamda, agglev, P, H, O)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mOsigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mPH_pearson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;31m# rhos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mPO_pearson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mHO_pearson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mpearsonr\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   3390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x and y must have length at least 2.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have length at least 2."
     ]
    }
   ],
   "source": [
    "## This iterates through all the sites in 'sitenames'\n",
    "for s in np.arange(len(sitenames)):\n",
    "    # 30min Precipitation Data\n",
    "    df_P30 = pd.read_excel(DATADIR+'PrecipData/'+str(sitenames[s])+'PrecipData.xlsx',index=False)\n",
    "    df_P30 = changeTimes.change_Pdata(df_P30)\n",
    "    \n",
    "    # Sum to Daily Precipitation Amounts\n",
    "    # checks for primary precip data\n",
    "    if 'priPrecipBulk' in df_P30:\n",
    "        df_P30b = df_P30.set_index('DateTime')\n",
    "        precip_daily = df_P30b['priPrecipBulk'].resample('D').sum() # sum total daily P\n",
    "        frac_year = df_P30b['FracYear'].resample('D').mean() # average daily FracYear (min or max better?)\n",
    "\n",
    "        daily_P = pd.DataFrame({'Total P':precip_daily,'FracYear':frac_year})\n",
    "        daily_P['Total P'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "        # Biweekly Stable Water Isotope Data\n",
    "        df_iso = pd.read_excel(DATADIR+'IsoData/'+str(sitenames[s])+'IsoData.xlsx',index=False)\n",
    "        df_iso = changeTimes.change_ISOdata(df_iso)\n",
    "\n",
    "\n",
    "\n",
    "        # Create Biweekly Precipitation Amount Timeseries to Correspond to Recorded Isotope Values\n",
    "        df_iso['setDate'] = pd.to_datetime(df_iso['setDate'])  \n",
    "        df_iso['collectDate'] = pd.to_datetime(df_iso['collectDate']) \n",
    "\n",
    "        P14 = []\n",
    "        for i in np.arange(len(df_iso['setDate'])):\n",
    "            subset = ((df_P30['DateTime'] > df_iso['setDate'].iloc[i]) \n",
    "                    & (df_P30['DateTime'] <= df_iso['collectDate'].iloc[i]))\n",
    "\n",
    "            df_sub = df_P30.loc[subset]\n",
    "            P14.append(np.nansum(df_sub['priPrecipBulk'].values))\n",
    "\n",
    "        df_iso['Total P'] = P14\n",
    "        del P14, i, subset\n",
    "    \n",
    "    # uses secondary precip data\n",
    "    else:\n",
    "        df_P30b = df_P30.set_index('DateTime')\n",
    "        precip_daily = df_P30b['secPrecipBulk'].resample('D').sum() # sum total daily P\n",
    "        frac_year = df_P30b['FracYear'].resample('D').mean() # average daily FracYear (min or max better?)\n",
    "\n",
    "        daily_P = pd.DataFrame({'Total P':precip_daily,'FracYear':frac_year})\n",
    "        daily_P['Total P'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "        # Biweekly Stable Water Isotope Data\n",
    "        df_iso = pd.read_excel(DATADIR+'IsoData/'+str(sitenames[s])+'IsoData.xlsx',index=False)\n",
    "        df_iso = changeTimes.change_ISOdata(df_iso)\n",
    "\n",
    "\n",
    "\n",
    "        # Create Biweekly Precipitation Amount Timeseries to Correspond to Recorded Isotope Values\n",
    "        df_iso['setDate'] = pd.to_datetime(df_iso['setDate'])  \n",
    "        df_iso['collectDate'] = pd.to_datetime(df_iso['collectDate']) \n",
    "\n",
    "        P14 = []\n",
    "        for i in np.arange(len(df_iso['setDate'])):\n",
    "            subset = ((df_P30['DateTime'] > df_iso['setDate'].iloc[i]) \n",
    "                    & (df_P30['DateTime'] <= df_iso['collectDate'].iloc[i]))\n",
    "\n",
    "            df_sub = df_P30.loc[subset]\n",
    "            P14.append(np.nansum(df_sub['secPrecipBulk'].values))\n",
    "\n",
    "        df_iso['Total P'] = P14\n",
    "        del P14, i, subset\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Step 1 - Remove seasonal time series component (Section 2.b.1)\n",
    "    '''\n",
    "    \n",
    "    # Define Sine Wave Functions\n",
    "    df_iso = df_iso.sort_values('DateTime')\n",
    "    df_iso = df_iso.dropna(subset=['Total P'])\n",
    "    tsX = df_iso['FracYear'].values \n",
    "    tsP = df_iso['Total P'].values\n",
    "    tsO = df_iso['d18OWater'].values\n",
    "    tsH = df_iso['d2HWater'].values   \n",
    "\n",
    "    dayslist = []\n",
    "    for dt in np.arange(len(df_iso['DateTime'])):\n",
    "        dayslist.append((df_iso['DateTime'].iloc[dt] - df_iso['DateTime'].iloc[0]).days)\n",
    "    dayslist = np.array(dayslist)\n",
    "\n",
    "    # lambda is precipitation frequency (see Eq. 4)\n",
    "    p_events = df_iso[df_iso['Total P'].notna()]\n",
    "    lamda = len(p_events['Total P'])/((daily_P.index.max() - daily_P.index.min()).days) \n",
    "\n",
    "    params1, params2 = iso_sine_signal.sine_params(sitenames[s],14,tsX,tsP,tsH,tsO) # 14 = biweekly sample, sample frequency\n",
    "    tsY_sine_wave = iso_sine_signal.sine_func(tsX, params1[0],params1[1],params1[2]) # solve for amplitude, phase, offset\n",
    "    adj_2H = np.array((tsH - tsY_sine_wave))    # remove seasonality from time series                       \n",
    "\n",
    "    tsY_sine_wave = iso_sine_signal.sine_func(tsX, params2[0],params2[1],params2[2])     \n",
    "    adj_18O = np.array((tsO - tsY_sine_wave))  \n",
    "    \n",
    "    # Get biweekly site stats of stochastic component\n",
    "    biweekly_stats = np.array([[np.mean(tsP), np.mean(adj_2H), np.mean(adj_18O)],\n",
    "                     [np.std(tsP), np.std(adj_2H), np.std(adj_18O)],\n",
    "                     [sp.stats.pearsonr(tsP,adj_2H)[0], sp.stats.pearsonr(tsP,adj_18O)[0], sp.stats.pearsonr(adj_2H,adj_18O)[0]]])\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Step 2 - Predict daily statistics from biweekly time series (Section 2.b.2) Now we will need to aggregate the \n",
    "    stochastic biweekly time series - i.e. calculated weighed running means at biweekly (14-day), 28-day, 42-day, \n",
    "    56-day and 84-day intervals    \n",
    "    '''\n",
    "\n",
    "    # function defining calculated statistics on time series\n",
    "    site_stats = [[sitenames[s], 14, lamda,\n",
    "                   biweekly_stats[0,0], biweekly_stats[0,1], biweekly_stats[0,2],\n",
    "                   biweekly_stats[1,0], biweekly_stats[1,1], biweekly_stats[1,2],\n",
    "                   biweekly_stats[2,0], biweekly_stats[2,1], biweekly_stats[2,2]]]\n",
    "    site_stats_check = len(site_stats)\n",
    "\n",
    "    for n in np.arange(28,85,14):\n",
    "        xday_Hb, xday_Pb, xday_Xb, days = getRunningMean.main(np.array(dayslist),np.array(tsH),np.array(tsP),tsX,n)\n",
    "        xday_Ob, xday_Pb, xday_Xb, days = getRunningMean.main(np.array(dayslist),np.array(tsO),np.array(tsP),tsX,n)  \n",
    "\n",
    "        xday_Xb = np.array(xday_Xb)\n",
    "        xday_Pb = np.array(xday_Pb)\n",
    "        xday_Hb = np.array(xday_Hb)\n",
    "        xday_Ob = np.array(xday_Ob)   \n",
    "\n",
    "        params1a, params2a = iso_sine_signal.sine_params(sitenames[s],n,xday_Xb,xday_Pb,xday_Hb,xday_Ob)\n",
    "\n",
    "        tsY_sine_wave = iso_sine_signal.sine_func(xday_Xb, params1a[0],params1a[1],params1a[2])\n",
    "        adj_2Hb = np.array((xday_Hb - tsY_sine_wave))                           \n",
    "\n",
    "        tsY_sine_wave = iso_sine_signal.sine_func(xday_Xb, params2a[0],params2a[1],params2a[2])     \n",
    "        adj_18Ob = np.array((xday_Ob - tsY_sine_wave))\n",
    "        \n",
    "        if len(xday_Pb)>2 and len(adj_2Hb)>2 and len(adj_18Ob)>2:\n",
    "            calcSiteStats.main(site_stats,sitenames[s],lamda,n,xday_Pb,adj_2Hb,adj_18Ob)\n",
    "            \n",
    "    if len(site_stats)==site_stats_check:\n",
    "        print(\"Site {} does not contain sufficient data.\".format(sitenames[s]))\n",
    "        \n",
    "    else:\n",
    "        ### the stats are labeled with 'B' here because they are of the stochastic component - not the original time series\n",
    "        Site_Stats = pd.DataFrame(site_stats, columns = ['site','agglev','lambda','PmuB','HmuB','OmuB','PsigB',\n",
    "                                                         'HsigB','OsigB','PHpB','POpB','HOpB'])\n",
    "    \n",
    "        ### Now we apply equation 4\n",
    "        Hi = [] ; Oi = [] # estimated 1-day parameters\n",
    "        H_a = [] ; O_a = [] # a term\n",
    "\n",
    "        xaxis = np.array(Site_Stats['agglev'].values)\n",
    "        yaxis1 = np.array(Site_Stats['HsigB'].values)\n",
    "        yaxis2 = np.array(Site_Stats['OsigB'].values)\n",
    "\n",
    "        def eq4(x,a,b):\n",
    "            return b/(x*lamda)**a\n",
    "\n",
    "        bounds = [[0.25,yaxis1[0]],[0.5,np.inf]]\n",
    "        p1,p2 = optimize.curve_fit(eq4, xaxis, yaxis1, p0 = [0.3,yaxis1[0]], bounds=bounds)\n",
    "\n",
    "        Hi.append(float(p1[1]))\n",
    "        H_a.append(p1[0])\n",
    "\n",
    "        bounds = [[0.2,yaxis2[0]],[0.5,np.inf]]\n",
    "        p1,p2 = optimize.curve_fit(eq4, xaxis, yaxis2,  p0 = [0.3,yaxis2[0]], bounds=bounds)\n",
    "\n",
    "        Oi.append(float(p1[1]))\n",
    "        O_a.append(p1[0])\n",
    "    \n",
    "        \n",
    "        '''\n",
    "        Step 3 - Generate daily time series with estimated statistcs (Section 2.b.2)\n",
    "        Step 4 - Add in seasonal time series component (Section 2.b.2)\n",
    "        '''\n",
    "    \n",
    "        H_scale = np.sort(np.array(adj_2H) * Hi / Site_Stats['HsigB'].iloc[0])\n",
    "        O_scale = np.sort(np.array(adj_18O) * Oi / Site_Stats['OsigB'].iloc[0])\n",
    "\n",
    "        copula_stats = np.matrix([[0, 0, 0], [np.std(tsP), Hi, Oi],\n",
    "                                    [Site_Stats['PHpB'].iloc[0], Site_Stats['POpB'].iloc[0],\n",
    "                                    Site_Stats['HOpB'].iloc[0]]])    \n",
    "\n",
    "        for num in np.arange(1,ensemble_size+1):\n",
    "            new_ts = conditional_copula_ts.main(daily_P['Total P'],copula_stats,H_scale,O_scale)\n",
    "            y = np.array([np.array(xi) for xi in new_ts])   \n",
    "\n",
    "            # Add back in n-day sine function here:\n",
    "            tsH_daily = y[:,0] + iso_sine_signal.sine_func(daily_P['FracYear'], params1[0],params1[1],params1[2])    \n",
    "            tsO_daily = y[:,1] + iso_sine_signal.sine_func(daily_P['FracYear'], params2[0],params2[1],params2[2])\n",
    "        \n",
    "            columnA = 'd2H_'+str(num)\n",
    "            columnB = 'd18O_'+str(num)\n",
    "            daily_P[columnA] = tsH_daily\n",
    "            daily_P[columnB] = tsO_daily \n",
    "\n",
    "        daily_P.to_csv(ROOTDIR+'/OUTPUT/'+sitenames[s]+'_daily_timeseries.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. visualize output - i.e. time series plots and dual isotope plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python or R scripts for plotting 1-2 sites (user can interchange site plotted by using unique site code (4 letter string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
