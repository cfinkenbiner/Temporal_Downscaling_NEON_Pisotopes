{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Precipitation and Isotope NEON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rpy2 so R scripts can run within jupyter notebook - run this one time\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "# This rpy2 'Rmagic' command allows to run the entire cell block as R code\n",
    "\n",
    "# Install R packages - run this one time\n",
    "install.packages(\"neonUtilities\")\n",
    "install.packages('openxlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load required libraries - run this every time\n",
    "library(neonUtilities)\n",
    "library(openxlsx)\n",
    "\n",
    "# creating a list of site names: this will iterate through all sites, or can be adapted to only download sites of interest\n",
    "siteNames <- c(\"DELA\", \"LENO\", \"TALL\", \"BARR\", \"BONA\", \"HEAL\", \"TOOL\", \"SRER\", \"SJER\", \"CPER\", \"NIWO\", \"RMNP\",\n",
    "             \"STER\", \"OSBS\", \"JERC\", \"PUUM\", \"KONZ\", \"UKFS\", \"SERC\", \"HARV\", \"UNDE\", \"BART\", \"NOGP\", \"WOOD\", \"OAES\",\n",
    "              \"GUAN\", \"GRSM\", \"ORNL\", \"CLBJ\", \"MOAB\", \"ONAQ\", \"BLAN\", \"MLBS\", \"SCBI\", \"WREF\", \"STEI\", \"YELL\")\n",
    "\n",
    "for (val in siteNames) {\n",
    "    siteName <- val\n",
    "    \n",
    "    # download isotope data\n",
    "    IsoData <- loadByProduct(dpID=\"DP1.00038.001\", site=siteName)\n",
    "    list2env(IsoData, .GlobalEnv)\n",
    "    destination <- paste(\"IsoData/\", siteName, \"IsoData.xlsx\", sep=\"\") # moves into IsoData folder and creates file name\n",
    "    write.xlsx(wdi_isoPerSample, destination, row.names=F)\n",
    "    \n",
    "    # download precip data - primary data if available, secondary data if not\n",
    "    PrecipData <- loadByProduct(dpID=\"DP1.00006.001\", site=siteName)\n",
    "    list2env(PrecipData, .GlobalEnv)\n",
    "    destination <- paste(\"PrecipData/\", siteName, \"PrecipData.xlsx\", sep=\"\") # moves into PrecipData folder and creates file name\n",
    "    if (exists(\"PRIPRE_30min\")) {\n",
    "        write.xlsx(PRIPRE_30min, destination, row.names=F)\n",
    "    } else {\n",
    "        write.xlsx(SECPRE_30min, destination, row.names=F)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate a downscaled daily data product at each NEON site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Import Statements\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOTDIR = os.getcwd() # Home directory\n",
    "CODEDIR = ROOTDIR + '/CODE/' # Code directory\n",
    "DATADIR = ROOTDIR + '/DATA/' # Data directory\n",
    "\n",
    "os.chdir(CODEDIR) # Change directory\n",
    "import changeTimes\n",
    "import getRunningMean\n",
    "import conditional_copula_ts\n",
    "import iso_sine_signal\n",
    "import calcSiteStats\n",
    "\n",
    "os.chdir(ROOTDIR) # Change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined variables\n",
    "sitenames = ['ONAQ']\n",
    "ensemble_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This iterates through all the sites in 'sitenames'\n",
    "for s in np.arange(len(sitenames)):\n",
    "    # 30min Precipitation Data\n",
    "    df_P30 = pd.read_excel(DATADIR+'PrecipData/'+str(sitenames[s])+'PrecipData.xlsx',index=False)\n",
    "    df_P30 = changeTimes.change_Pdata(df_P30)\n",
    "\n",
    "\n",
    "    # Sum to Daily Precipitation Amounts\n",
    "    df_P30b = df_P30.set_index('DateTime')\n",
    "    precip_daily = df_P30b['priPrecipBulk'].resample('D').sum() # sum total daily P\n",
    "    frac_year = df_P30b['FracYear'].resample('D').mean() # average daily FracYear (min or max better?)\n",
    "\n",
    "    daily_P = pd.DataFrame({'Total P':precip_daily,'FracYear':frac_year})\n",
    "    daily_P['Total P'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "    # Biweekly Stable Water Isotope Data\n",
    "    df_iso = pd.read_excel(DATADIR+'IsoData/'+str(sitenames[s])+'IsoData.xlsx',index=False)\n",
    "    df_iso = changeTimes.change_ISOdata(df_iso)\n",
    "\n",
    "\n",
    "\n",
    "    # Create Biweekly Precipitation Amount Timeseries to Correspond to Recorded Isotope Values\n",
    "    df_iso['setDate'] = pd.to_datetime(df_iso['setDate'])  \n",
    "    df_iso['collectDate'] = pd.to_datetime(df_iso['collectDate']) \n",
    "\n",
    "    P14 = []\n",
    "    for i in np.arange(len(df_iso['setDate'])):\n",
    "        subset = ((df_P30['DateTime'] > df_iso['setDate'].iloc[i]) \n",
    "                & (df_P30['DateTime'] <= df_iso['collectDate'].iloc[i]))\n",
    "\n",
    "        df_sub = df_P30.loc[subset]\n",
    "        P14.append(np.nansum(df_sub['priPrecipBulk'].values))\n",
    "\n",
    "    df_iso['Total P'] = P14\n",
    "    del P14, i, subset\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Step 1 - Remove seasonal time series component (Section 2.b.1)\n",
    "    '''\n",
    "    \n",
    "    # Define Sine Wave Functions\n",
    "    df_iso = df_iso.sort_values('DateTime')\n",
    "    df_iso = df_iso.dropna(subset=['Total P'])\n",
    "    tsX = df_iso['FracYear'].values \n",
    "    tsP = df_iso['Total P'].values\n",
    "    tsO = df_iso['d18OWater'].values\n",
    "    tsH = df_iso['d2HWater'].values   \n",
    "\n",
    "    dayslist = []\n",
    "    for dt in np.arange(len(df_iso['DateTime'])):\n",
    "        dayslist.append((df_iso['DateTime'].iloc[dt] - df_iso['DateTime'].iloc[0]).days)\n",
    "    dayslist = np.array(dayslist)\n",
    "\n",
    "    # lambda is precipitation frequency (see Eq. 4)\n",
    "    p_events = df_iso[df_iso['Total P'].notna()]\n",
    "    lamda = len(p_events['Total P'])/((daily_P.index.max() - daily_P.index.min()).days) \n",
    "\n",
    "    params1, params2 = iso_sine_signal.sine_params(sitenames[s],14,tsX,tsP,tsH,tsO) # 14 = biweekly sample, sample frequency\n",
    "    tsY_sine_wave = iso_sine_signal.sine_func(tsX, params1[0],params1[1],params1[2]) # solve for amplitude, phase, offset\n",
    "    adj_2H = np.array((tsH - tsY_sine_wave))    # remove seasonality from time series                       \n",
    "\n",
    "    tsY_sine_wave = iso_sine_signal.sine_func(tsX, params2[0],params2[1],params2[2])     \n",
    "    adj_18O = np.array((tsO - tsY_sine_wave))  \n",
    "    \n",
    "    # Get biweekly site stats of stochastic component\n",
    "    biweekly_stats = np.array([[np.mean(tsP), np.mean(adj_2H), np.mean(adj_18O)],\n",
    "                     [np.std(tsP), np.std(adj_2H), np.std(adj_18O)],\n",
    "                     [sp.stats.pearsonr(tsP,adj_2H)[0], sp.stats.pearsonr(tsP,adj_18O)[0], sp.stats.pearsonr(adj_2H,adj_18O)[0]]])\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Step 2 - Predict daily statistics from biweekly time series (Section 2.b.2) Now we will need to aggregate the \n",
    "    stochastic biweekly time series - i.e. calculated weighed running means at biweekly (14-day), 28-day, 42-day, \n",
    "    56-day and 84-day intervals    \n",
    "    '''\n",
    "\n",
    "    # function defining calculated statistics on time series\n",
    "    site_stats = [[sitenames[s], 14, lamda,\n",
    "                   biweekly_stats[0,0], biweekly_stats[0,1], biweekly_stats[0,2],\n",
    "                   biweekly_stats[1,0], biweekly_stats[1,1], biweekly_stats[1,2],\n",
    "                   biweekly_stats[2,0], biweekly_stats[2,1], biweekly_stats[2,2]]]\n",
    "\n",
    "    for n in np.arange(28,85,14):\n",
    "        xday_Hb, xday_Pb, xday_Xb, days = getRunningMean.main(np.array(dayslist),np.array(tsH),np.array(tsP),tsX,n)\n",
    "        xday_Ob, xday_Pb, xday_Xb, days = getRunningMean.main(np.array(dayslist),np.array(tsO),np.array(tsP),tsX,n)  \n",
    "\n",
    "        xday_Xb = np.array(xday_Xb)\n",
    "        xday_Pb = np.array(xday_Pb)\n",
    "        xday_Hb = np.array(xday_Hb)\n",
    "        xday_Ob = np.array(xday_Ob)   \n",
    "\n",
    "        params1a, params2a = iso_sine_signal.sine_params(sitenames[s],n,xday_Xb,xday_Pb,xday_Hb,xday_Ob)\n",
    "\n",
    "        tsY_sine_wave = iso_sine_signal.sine_func(xday_Xb, params1a[0],params1a[1],params1a[2])\n",
    "        adj_2Hb = np.array((xday_Hb - tsY_sine_wave))                           \n",
    "\n",
    "        tsY_sine_wave = iso_sine_signal.sine_func(xday_Xb, params2a[0],params2a[1],params2a[2])     \n",
    "        adj_18Ob = np.array((xday_Ob - tsY_sine_wave))   \n",
    "\n",
    "        calcSiteStats.main(site_stats,sitenames[s],lamda,n,xday_Pb,adj_2Hb,adj_18Ob)\n",
    "\n",
    "    ### the stats are labeled with 'B' here because they are of the stochastic component - not the original time series\n",
    "    Site_Stats = pd.DataFrame(site_stats, columns = ['site','agglev','lambda','PmuB','HmuB','OmuB','PsigB',\n",
    "                                                     'HsigB','OsigB','PHpB','POpB','HOpB'])\n",
    "    \n",
    "    ### Now we apply equation 4\n",
    "    Hi = [] ; Oi = [] # estimated 1-day parameters\n",
    "    H_a = [] ; O_a = [] # a term\n",
    "\n",
    "    xaxis = np.array(Site_Stats['agglev'].values)\n",
    "    yaxis1 = np.array(Site_Stats['HsigB'].values)\n",
    "    yaxis2 = np.array(Site_Stats['OsigB'].values)\n",
    "\n",
    "    def eq4(x,a,b):\n",
    "        return b/(x*lamda)**a\n",
    "\n",
    "    bounds = [[0.25,yaxis1[0]],[0.5,np.inf]]\n",
    "    p1,p2 = optimize.curve_fit(eq4, xaxis, yaxis1, p0 = [0.3,yaxis1[0]], bounds=bounds)\n",
    "\n",
    "    Hi.append(float(p1[1]))\n",
    "    H_a.append(p1[0])\n",
    "\n",
    "    bounds = [[0.2,yaxis2[0]],[0.5,np.inf]]\n",
    "    p1,p2 = optimize.curve_fit(eq4, xaxis, yaxis2,  p0 = [0.3,yaxis2[0]], bounds=bounds)\n",
    "\n",
    "    Oi.append(float(p1[1]))\n",
    "    O_a.append(p1[0])\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Step 3 - Generate daily time series with estimated statistcs (Section 2.b.2)\n",
    "    Step 4 - Add in seasonal time series component (Section 2.b.2)\n",
    "    '''\n",
    "    \n",
    "    H_scale = np.sort(np.array(adj_2H) * Hi / Site_Stats['HsigB'].iloc[0])\n",
    "    O_scale = np.sort(np.array(adj_18O) * Oi / Site_Stats['OsigB'].iloc[0])\n",
    "\n",
    "    copula_stats = np.matrix([[0, 0, 0], [np.std(tsP), Hi, Oi],\n",
    "                              [Site_Stats['PHpB'].iloc[0], Site_Stats['POpB'].iloc[0],\n",
    "                               Site_Stats['HOpB'].iloc[0]]])    \n",
    "\n",
    "    for num in np.arange(1,ensemble_size+1):\n",
    "        new_ts = conditional_copula_ts.main(daily_P['Total P'],copula_stats,H_scale,O_scale)\n",
    "        y = np.array([np.array(xi) for xi in new_ts])   \n",
    "\n",
    "        # Add back in n-day sine function here:\n",
    "        tsH_daily = y[:,0] + iso_sine_signal.sine_func(daily_P['FracYear'], params1[0],params1[1],params1[2])    \n",
    "        tsO_daily = y[:,1] + iso_sine_signal.sine_func(daily_P['FracYear'], params2[0],params2[1],params2[2])\n",
    "        \n",
    "        columnA = 'd2H_'+str(num)\n",
    "        columnB = 'd18O_'+str(num)\n",
    "        daily_P[columnA] = tsH_daily\n",
    "        daily_P[columnB] = tsO_daily \n",
    "\n",
    "    daily_P.to_csv(ROOTDIR+'/OUTPUT/'+sitenames[s]+'_daily_timeseries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. visualize output - i.e. time series plots and dual isotope plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python or R scripts for plotting 1-2 sites (user can interchange site plotted by using unique site code (4 letter string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
