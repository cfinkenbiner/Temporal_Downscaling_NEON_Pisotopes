{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Precipitation and Isotope NEON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rpy2 so R scripts can run within jupyter notebook - run this one time\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "# This rpy2 'Rmagic' command allows to run the entire cell block as R code\n",
    "\n",
    "# Install R packages - run this one time\n",
    "install.packages(\"neonUtilities\")\n",
    "install.packages('openxlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Load required libraries - run this every time\n",
    "library(neonUtilities)\n",
    "library(openxlsx)\n",
    "\n",
    "# creating a list of site names: this will iterate through all sites, or can be adapted to only download sites of interest\n",
    "siteNames <- c(\"DELA\", \"LENO\", \"TALL\", \"BARR\", \"BONA\", \"HEAL\", \"TOOL\", \"SRER\", \"SJER\", \"CPER\", \"NIWO\", \"RMNP\",\n",
    "             \"STER\", \"OSBS\", \"JERC\", \"PUUM\", \"KONZ\", \"UKFS\", \"SERC\", \"HARV\", \"UNDE\", \"BART\", \"NOGP\", \"WOOD\", \"OAES\",\n",
    "              \"GUAN\", \"GRSM\", \"ORNL\", \"CLBJ\", \"MOAB\", \"ONAQ\", \"BLAN\", \"MLBS\", \"SCBI\", \"WREF\", \"STEI\", \"YELL\")\n",
    "\n",
    "for (val in siteNames) {\n",
    "    siteName <- val\n",
    "    \n",
    "    # download isotope data\n",
    "    IsoData <- loadByProduct(dpID=\"DP1.00038.001\", site=siteName)\n",
    "    list2env(IsoData, .GlobalEnv)\n",
    "    destination <- paste(\"IsoData/\", siteName, \"IsoData.xlsx\", sep=\"\") # moves into IsoData folder and creates file name\n",
    "    write.xlsx(wdi_isoPerSample, destination, row.names=F)\n",
    "    \n",
    "    # download precip data - primary data if available, secondary data if not\n",
    "    PrecipData <- loadByProduct(dpID=\"DP1.00006.001\", site=siteName)\n",
    "    list2env(PrecipData, .GlobalEnv)\n",
    "    destination <- paste(\"PrecipData/\", siteName, \"PrecipData.xlsx\", sep=\"\") # moves into PrecipData folder and creates file name\n",
    "    if (exists(\"PRIPRE_30min\")) {\n",
    "        write.xlsx(PRIPRE_30min, destination, row.names=F)\n",
    "    } else {\n",
    "        write.xlsx(SECPRE_30min, destination, row.names=F)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate a downscaled daily data product at each NEON site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python Import Statements\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "ROOTDIR = os.getcwd() # Home directory\n",
    "CODEDIR = ROOTDIR + '/CODE/' # Code directory\n",
    "DATADIR = ROOTDIR + '/DATA/' # Data directory\n",
    "\n",
    "os.chdir(CODEDIR) # Change directory\n",
    "import changeTimes\n",
    "import getRunningMean\n",
    "import conditional_copula_ts\n",
    "import iso_sine_signal\n",
    "import calcSiteStats\n",
    "\n",
    "os.chdir(ROOTDIR) # Change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined variables\n",
    "sitenames = [\"DELA\", \"LENO\", \"TALL\", \"BONA\", \"HEAL\", \"TOOL\", \"SRER\", \"SJER\", \"CPER\", \"NIWO\", \"RMNP\", \"STER\",\n",
    "             \"OSBS\", \"JERC\", \"PUUM\", \"KONZ\", \"UKFS\", \"SERC\", \"HARV\", \"UNDE\", \"BART\", \"NOGP\", \"WOOD\", \"OAES\",\n",
    "             \"GUAN\", \"GRSM\", \"ORNL\", \"CLBJ\", \"MOAB\", \"ONAQ\", \"BLAN\", \"MLBS\", \"SCBI\", \"WREF\", \"STEI\", \"YELL\"]\n",
    "ensemble_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n",
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/scipy/optimize/minpack.py:799: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  category=OptimizeWarning)\n"
     ]
    }
   ],
   "source": [
    "## This iterates through all the sites in 'sitenames'\n",
    "amplitude = []\n",
    "correlation = []\n",
    "\n",
    "for s in np.arange(len(sitenames)):\n",
    "    # 30min Precipitation Data\n",
    "    df_P30 = pd.read_excel(DATADIR+'PrecipData/'+str(sitenames[s])+'PrecipData.xlsx',index=False)\n",
    "    df_P30 = changeTimes.change_Pdata(df_P30)\n",
    "    \n",
    "    # Sum to Daily Precipitation Amounts\n",
    "    # checks for primary precip data\n",
    "    if 'priPrecipBulk' in df_P30:\n",
    "        df_P30b = df_P30.set_index('DateTime')\n",
    "        precip_daily = df_P30b['priPrecipBulk'].resample('D').sum() # sum total daily P\n",
    "        frac_year = df_P30b['FracYear'].resample('D').mean() # average daily FracYear (min or max better?)\n",
    "\n",
    "        daily_P = pd.DataFrame({'Total P':precip_daily,'FracYear':frac_year})\n",
    "        daily_P['Total P'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "        # Biweekly Stable Water Isotope Data\n",
    "        df_iso = pd.read_excel(DATADIR+'IsoData/'+str(sitenames[s])+'IsoData.xlsx',index=False)\n",
    "        df_iso = changeTimes.change_ISOdata(df_iso)\n",
    "\n",
    "\n",
    "\n",
    "        # Create Biweekly Precipitation Amount Timeseries to Correspond to Recorded Isotope Values\n",
    "        df_iso['setDate'] = pd.to_datetime(df_iso['setDate'])  \n",
    "        df_iso['collectDate'] = pd.to_datetime(df_iso['collectDate']) \n",
    "\n",
    "        P14 = []\n",
    "        for i in np.arange(len(df_iso['setDate'])):\n",
    "            subset = ((df_P30['DateTime'] > df_iso['setDate'].iloc[i]) \n",
    "                    & (df_P30['DateTime'] <= df_iso['collectDate'].iloc[i]))\n",
    "\n",
    "            df_sub = df_P30.loc[subset]\n",
    "            P14.append(np.nansum(df_sub['priPrecipBulk'].values))\n",
    "\n",
    "        df_iso['Total P'] = P14\n",
    "        del P14, i, subset\n",
    "    \n",
    "    # uses secondary precip data\n",
    "    else:\n",
    "        df_P30b = df_P30.set_index('DateTime')\n",
    "        precip_daily = df_P30b['secPrecipBulk'].resample('D').sum() # sum total daily P\n",
    "        frac_year = df_P30b['FracYear'].resample('D').mean() # average daily FracYear (min or max better?)\n",
    "\n",
    "        daily_P = pd.DataFrame({'Total P':precip_daily,'FracYear':frac_year})\n",
    "        daily_P['Total P'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "        # Biweekly Stable Water Isotope Data\n",
    "        df_iso = pd.read_excel(DATADIR+'IsoData/'+str(sitenames[s])+'IsoData.xlsx',index=False)\n",
    "        df_iso = changeTimes.change_ISOdata(df_iso)\n",
    "\n",
    "\n",
    "\n",
    "        # Create Biweekly Precipitation Amount Timeseries to Correspond to Recorded Isotope Values\n",
    "        df_iso['setDate'] = pd.to_datetime(df_iso['setDate'])  \n",
    "        df_iso['collectDate'] = pd.to_datetime(df_iso['collectDate']) \n",
    "\n",
    "        P14 = []\n",
    "        for i in np.arange(len(df_iso['setDate'])):\n",
    "            subset = ((df_P30['DateTime'] > df_iso['setDate'].iloc[i]) \n",
    "                    & (df_P30['DateTime'] <= df_iso['collectDate'].iloc[i]))\n",
    "\n",
    "            df_sub = df_P30.loc[subset]\n",
    "            P14.append(np.nansum(df_sub['secPrecipBulk'].values))\n",
    "\n",
    "        df_iso['Total P'] = P14\n",
    "        del P14, i, subset\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Step 1 - Remove seasonal time series component (Section 2.b.1)\n",
    "    '''\n",
    "    \n",
    "    # Define Sine Wave Functions\n",
    "    df_iso = df_iso.sort_values('DateTime')\n",
    "    df_iso = df_iso.dropna(subset=['Total P'])\n",
    "    tsX = df_iso['FracYear'].values \n",
    "    tsP = df_iso['Total P'].values\n",
    "    tsO = df_iso['d18OWater'].values\n",
    "    tsH = df_iso['d2HWater'].values   \n",
    "\n",
    "    dayslist = []\n",
    "    for dt in np.arange(len(df_iso['DateTime'])):\n",
    "        dayslist.append((df_iso['DateTime'].iloc[dt] - df_iso['DateTime'].iloc[0]).days)\n",
    "    dayslist = np.array(dayslist)\n",
    "\n",
    "    # lambda is precipitation frequency (see Eq. 4)\n",
    "    p_events = df_iso[df_iso['Total P'].notna()]\n",
    "    lamda = len(p_events['Total P'])/((daily_P.index.max() - daily_P.index.min()).days) \n",
    "\n",
    "    params1, params2 = iso_sine_signal.sine_params(sitenames[s],14,tsX,tsP,tsH,tsO) # 14 = biweekly sample, sample frequency\n",
    "    tsY_sine_wave = iso_sine_signal.sine_func(tsX, params1[0],params1[1],params1[2]) # solve for amplitude, phase, offset\n",
    "    adj_2H = np.array((tsH - tsY_sine_wave))    # remove seasonality from time series                       \n",
    "\n",
    "    tsY_sine_wave = iso_sine_signal.sine_func(tsX, params2[0],params2[1],params2[2])     \n",
    "    adj_18O = np.array((tsO - tsY_sine_wave)) \n",
    "    \n",
    "    amplitude.append(params1[0])\n",
    "    \n",
    "    # Get biweekly site stats of stochastic component\n",
    "    biweekly_stats = np.array([[np.mean(tsP), np.mean(adj_2H), np.mean(adj_18O)],\n",
    "                     [np.std(tsP), np.std(adj_2H), np.std(adj_18O)],\n",
    "                     [sp.stats.pearsonr(tsP,adj_2H)[0], sp.stats.pearsonr(tsP,adj_18O)[0], sp.stats.pearsonr(adj_2H,adj_18O)[0]]])\n",
    "\n",
    "    correlation.append(sp.stats.pearsonr(tsP,adj_18O)[0])\n",
    "    \n",
    "    '''\n",
    "    Step 2 - Predict daily statistics from biweekly time series (Section 2.b.2) Now we will need to aggregate the \n",
    "    stochastic biweekly time series - i.e. calculated weighed running means at biweekly (14-day), 28-day, 42-day, \n",
    "    56-day and 84-day intervals    \n",
    "    '''\n",
    "\n",
    "    # function defining calculated statistics on time series\n",
    "    site_stats = [[sitenames[s], 14, lamda,\n",
    "                   biweekly_stats[0,0], biweekly_stats[0,1], biweekly_stats[0,2],\n",
    "                   biweekly_stats[1,0], biweekly_stats[1,1], biweekly_stats[1,2],\n",
    "                   biweekly_stats[2,0], biweekly_stats[2,1], biweekly_stats[2,2]]]\n",
    "    site_stats_check = len(site_stats)\n",
    "\n",
    "    for n in np.arange(28,85,14):\n",
    "        xday_Hb, xday_Pb, xday_Xb, days = getRunningMean.main(np.array(dayslist),np.array(tsH),np.array(tsP),tsX,n)\n",
    "        xday_Ob, xday_Pb, xday_Xb, days = getRunningMean.main(np.array(dayslist),np.array(tsO),np.array(tsP),tsX,n)  \n",
    "\n",
    "        xday_Xb = np.array(xday_Xb)\n",
    "        xday_Pb = np.array(xday_Pb)\n",
    "        xday_Hb = np.array(xday_Hb)\n",
    "        xday_Ob = np.array(xday_Ob)   \n",
    "\n",
    "        params1a, params2a = iso_sine_signal.sine_params(sitenames[s],n,xday_Xb,xday_Pb,xday_Hb,xday_Ob)\n",
    "\n",
    "        tsY_sine_wave = iso_sine_signal.sine_func(xday_Xb, params1a[0],params1a[1],params1a[2])\n",
    "        adj_2Hb = np.array((xday_Hb - tsY_sine_wave))                           \n",
    "\n",
    "        tsY_sine_wave = iso_sine_signal.sine_func(xday_Xb, params2a[0],params2a[1],params2a[2])     \n",
    "        adj_18Ob = np.array((xday_Ob - tsY_sine_wave))\n",
    "        \n",
    "        if len(xday_Pb)>2 and len(adj_2Hb)>2 and len(adj_18Ob)>2:\n",
    "            calcSiteStats.main(site_stats,sitenames[s],lamda,n,xday_Pb,adj_2Hb,adj_18Ob)\n",
    "            \n",
    "    if len(site_stats)==site_stats_check:\n",
    "        print(\"Site {} does not contain sufficient data.\".format(sitenames[s]))\n",
    "        \n",
    "    else:\n",
    "        ### the stats are labeled with 'B' here because they are of the stochastic component - not the original time series\n",
    "        Site_Stats = pd.DataFrame(site_stats, columns = ['site','agglev','lambda','PmuB','HmuB','OmuB','PsigB',\n",
    "                                                         'HsigB','OsigB','PHpB','POpB','HOpB'])\n",
    "    \n",
    "        ### Now we apply equation 4\n",
    "        Hi = [] ; Oi = [] # estimated 1-day parameters\n",
    "        H_a = [] ; O_a = [] # a term\n",
    "\n",
    "        xaxis = np.array(Site_Stats['agglev'].values)\n",
    "        yaxis1 = np.array(Site_Stats['HsigB'].values)\n",
    "        yaxis2 = np.array(Site_Stats['OsigB'].values)\n",
    "\n",
    "        def eq4(x,a,b):\n",
    "            return b/(x*lamda)**a\n",
    "\n",
    "        bounds = [[0.2,yaxis1[0]],[0.5,np.inf]]\n",
    "        p1,p2 = optimize.curve_fit(eq4, xaxis, yaxis1, p0 = [0.3,yaxis1[0]], bounds=bounds)\n",
    "\n",
    "        Hi.append(float(p1[1]))\n",
    "        H_a.append(p1[0])\n",
    "\n",
    "        bounds = [[0.2,yaxis2[0]],[0.5,np.inf]]\n",
    "        p1,p2 = optimize.curve_fit(eq4, xaxis, yaxis2,  p0 = [0.3,yaxis2[0]], bounds=bounds)\n",
    "\n",
    "        Oi.append(float(p1[1]))\n",
    "        O_a.append(p1[0])\n",
    "    \n",
    "        \n",
    "        '''\n",
    "        Step 3 - Generate daily time series with estimated statistcs (Section 2.b.2)\n",
    "        Step 4 - Add in seasonal time series component (Section 2.b.2)\n",
    "        '''\n",
    "    \n",
    "        H_scale = np.sort(np.array(adj_2H) * Hi / Site_Stats['HsigB'].iloc[0])\n",
    "        O_scale = np.sort(np.array(adj_18O) * Oi / Site_Stats['OsigB'].iloc[0])\n",
    "\n",
    "        copula_stats = np.matrix([[0, 0, 0], [np.std(tsP), Hi, Oi],\n",
    "                                    [Site_Stats['PHpB'].iloc[0], \n",
    "                                     Site_Stats['POpB'].iloc[0],\n",
    "                                     Site_Stats['HOpB'].iloc[0]]])    \n",
    "\n",
    "        for num in np.arange(1,ensemble_size+1):\n",
    "            new_ts = conditional_copula_ts.main(daily_P['Total P'],copula_stats,H_scale,O_scale)\n",
    "            y = np.array([np.array(xi) for xi in new_ts])   \n",
    "\n",
    "            # Add back in n-day sine function here:\n",
    "            tsH_daily = y[:,0] + iso_sine_signal.sine_func(daily_P['FracYear'], params1[0],params1[1],params1[2])    \n",
    "            tsO_daily = y[:,1] + iso_sine_signal.sine_func(daily_P['FracYear'], params2[0],params2[1],params2[2])\n",
    "        \n",
    "            columnA = 'd2H_'+str(num)\n",
    "            columnB = 'd18O_'+str(num)\n",
    "            daily_P[columnA] = tsH_daily\n",
    "            daily_P[columnB] = tsO_daily \n",
    "\n",
    "        daily_P.to_csv(ROOTDIR+'/OUTPUT/'+sitenames[s]+'_daily_timeseries.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. visualize output - i.e. time series plots and dual isotope plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sort data to confirm statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     uid            namedLocation  \\\n",
      "0   ea6d24f0-ad85-4251-b2f1-5d0c0f2d68dc  YELL.TOS.wet.deposition   \n",
      "1   59c2f3b5-bf57-47b0-91ab-d8cf759b6133  YELL.TOS.wet.deposition   \n",
      "2   b17073e8-a2e5-45c9-9f06-182bc562aa5e  YELL.TOS.wet.deposition   \n",
      "3   93450bd8-0a75-4677-9331-08a2291902c6  YELL.TOS.wet.deposition   \n",
      "4   560ee57e-5c67-419a-bc32-451ee9072f9a  YELL.TOS.wet.deposition   \n",
      "5   06c3b253-ca30-4220-8d2c-d08fa89b8baf  YELL.TOS.wet.deposition   \n",
      "6   23f0b691-ffab-4ebb-8472-50f18628ff54  YELL.TOS.wet.deposition   \n",
      "7   41b9e252-8c14-402a-8d0b-84d3dce949e7  YELL.TOS.wet.deposition   \n",
      "8   6fe7abda-36b4-4838-aef1-463f6b46d8ba  YELL.TOS.wet.deposition   \n",
      "9   e2c93b0f-0d77-4973-be5e-573355835c38  YELL.TOS.wet.deposition   \n",
      "10  8b7a32d5-fb4c-4499-a8ad-521bbf7062a4  YELL.TOS.wet.deposition   \n",
      "11  f8e90908-66b6-4128-9f21-5c5b3245d726  YELL.TOS.wet.deposition   \n",
      "12  112d9491-c5b8-4640-b1de-778ff8563235  YELL.TOS.wet.deposition   \n",
      "13  fc03ac6c-c09e-40e6-a567-b6931cb2879b  YELL.TOS.wet.deposition   \n",
      "\n",
      "                  sampleID  sampleCode               isoTestSubsampleID  \\\n",
      "0   WDP.YELL.20190724.1017         NaN  WDP.YELL.20190724.1017.ISO.TEST   \n",
      "1   WDP.YELL.20190806.1155         NaN  WDP.YELL.20190806.1155.ISO.TEST   \n",
      "2   WDP.YELL.20190821.0942         NaN  WDP.YELL.20190821.0942.ISO.TEST   \n",
      "3   WDP.YELL.20190905.1000         NaN  WDP.YELL.20190905.1000.ISO.TEST   \n",
      "4   WDP.YELL.20190917.1000         NaN  WDP.YELL.20190917.1000.ISO.TEST   \n",
      "5   WDP.YELL.20191002.1126         NaN  WDP.YELL.20191002.1126.ISO.TEST   \n",
      "6   WDP.YELL.20191016.1010         NaN  WDP.YELL.20191016.1010.ISO.TEST   \n",
      "7   WDP.YELL.20191030.1020         NaN  WDP.YELL.20191030.1020.ISO.TEST   \n",
      "8   WDP.YELL.20191111.1020         NaN  WDP.YELL.20191111.1020.ISO.TEST   \n",
      "9   WDP.YELL.20191126.1120         NaN  WDP.YELL.20191126.1120.ISO.TEST   \n",
      "10  WDP.YELL.20191220.1106         NaN  WDP.YELL.20191220.1106.ISO.TEST   \n",
      "11  WDP.YELL.20191210.1130         NaN  WDP.YELL.20191210.1130.ISO.TEST   \n",
      "12  WDP.YELL.20200122.1106         NaN  WDP.YELL.20200122.1106.ISO.TEST   \n",
      "13  WDP.YELL.20200128.1113         NaN  WDP.YELL.20200128.1113.ISO.TEST   \n",
      "\n",
      "   isoTestSubsampleBarcode             setDate         collectDate  d18OWater  \\\n",
      "0             A00000054543 2019-07-10 16:56:00 2019-07-24 16:17:00    -12.982   \n",
      "1             A00000054696 2019-07-24 16:52:00 2019-08-06 17:55:00     -7.214   \n",
      "2             A00000054510 2019-08-06 18:07:00 2019-08-21 15:42:00     -8.783   \n",
      "3             A00000114163 2019-08-21 16:23:00 2019-09-05 16:00:00     -8.276   \n",
      "4             A00000114136 2019-09-09 16:23:00 2019-09-17 16:00:00     -8.280   \n",
      "5             A00000115555 2019-09-17 16:03:00 2019-10-02 17:26:00    -16.256   \n",
      "6             A00000114411 2019-10-02 17:37:00 2019-10-16 16:10:00    -16.118   \n",
      "7             A00000114441 2019-10-16 16:37:00 2019-10-30 16:20:00    -18.037   \n",
      "8             A00000115300 2019-10-30 16:24:00 2019-11-11 17:20:00    -22.762   \n",
      "9             A00000115000 2019-11-11 17:25:00 2019-11-26 18:20:00    -13.274   \n",
      "10            A00000115411 2018-12-12 18:00:00 2019-12-20 18:06:00    -19.486   \n",
      "11            A00000115150 2019-11-26 18:20:00 2019-12-10 18:30:00    -21.869   \n",
      "12            A00000115400 2019-12-20 18:11:00 2020-01-22 18:06:00    -19.434   \n",
      "13            A00000054528 2020-01-22 18:10:00 2020-01-28 18:13:00    -16.474   \n",
      "\n",
      "    d2HWater  ...        analysisDate  sampleCondition externalRemarks  \\\n",
      "0   -108.539  ... 2019-09-27 18:00:00               OK             NaN   \n",
      "1    -60.121  ... 2019-09-25 18:00:00               OK             NaN   \n",
      "2    -64.234  ... 2019-12-06 19:00:00               OK             NaN   \n",
      "3    -82.746  ... 2019-12-06 19:00:00               OK             NaN   \n",
      "4    -62.963  ... 2019-12-06 19:00:00               OK             NaN   \n",
      "5   -119.315  ... 2019-12-06 19:00:00               OK             NaN   \n",
      "6   -121.013  ... 2020-01-06 19:00:00               OK             NaN   \n",
      "7   -139.711  ... 2020-01-06 19:00:00               OK             NaN   \n",
      "8   -185.769  ... 2020-01-06 19:00:00               OK             NaN   \n",
      "9   -101.961  ... 2020-01-06 19:00:00               OK             NaN   \n",
      "10  -156.249  ... 2020-02-25 19:00:00               OK             NaN   \n",
      "11  -170.336  ... 2020-02-25 19:00:00               OK             NaN   \n",
      "12  -155.334  ... 2020-02-25 19:00:00               OK             NaN   \n",
      "13  -126.512  ... 2020-02-25 19:00:00               OK             NaN   \n",
      "\n",
      "     publicationDate            DateTime  FracYear Total P Total P_2  \\\n",
      "0   20200518T180032Z 2019-07-24 16:17:00  0.561259   18.42     18.23   \n",
      "1   20200518T154404Z 2019-08-06 17:55:00  0.596851   20.23     20.22   \n",
      "2   20200518T154404Z 2019-08-21 15:42:00  0.637919   21.43     21.57   \n",
      "3   20200518T160143Z 2019-09-05 16:00:00  0.678987   13.00     12.80   \n",
      "4   20200518T160143Z 2019-09-17 16:00:00  0.711841    5.96      5.97   \n",
      "5   20200518T160659Z 2019-10-02 17:26:00  0.752909   31.39     31.54   \n",
      "6   20200518T160659Z 2019-10-16 16:10:00  0.791239   18.20     18.15   \n",
      "7   20200518T160659Z 2019-10-30 16:20:00  0.829569   23.28     23.42   \n",
      "8   20200518T153602Z 2019-11-11 17:20:00  0.862423    9.54      9.38   \n",
      "9   20200518T153602Z 2019-11-26 18:20:00  0.903491   11.03     10.93   \n",
      "10  20200518T153857Z 2019-12-20 18:06:00  0.969199  451.46    451.41   \n",
      "11  20200518T153857Z 2019-12-10 18:30:00  0.941821   28.38     28.36   \n",
      "12  20200518T180204Z 2020-01-22 18:06:00  0.060233   28.81     28.89   \n",
      "13  20200518T180204Z 2020-01-28 18:13:00  0.076660    5.41      5.22   \n",
      "\n",
      "    Number of Days  Days of Precip  \n",
      "0               14              14  \n",
      "1               13              13  \n",
      "2               15              15  \n",
      "3               15              15  \n",
      "4                8               8  \n",
      "5               15              15  \n",
      "6               14              14  \n",
      "7               14              14  \n",
      "8               12              12  \n",
      "9               15              15  \n",
      "10             373             356  \n",
      "11              14              14  \n",
      "12              33              33  \n",
      "13               6               6  \n",
      "\n",
      "[14 rows x 23 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-242d59108492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_iso\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# uses secondary precip data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "# Site list\n",
    "#sitenames = [\"DELA\", \"LENO\", \"TALL\", \"BONA\", \"HEAL\", \"TOOL\", \"SRER\", \"SJER\", \"CPER\", \"NIWO\", \"RMNP\", \"STER\",\n",
    "#             \"OSBS\", \"JERC\", \"PUUM\", \"KONZ\", \"UKFS\", \"SERC\", \"HARV\", \"UNDE\", \"BART\", \"NOGP\", \"WOOD\", \"OAES\",\n",
    "#             \"GUAN\", \"GRSM\", \"ORNL\", \"CLBJ\", \"MOAB\", \"ONAQ\", \"BLAN\", \"MLBS\", \"SCBI\", \"WREF\", \"STEI\", \"YELL\"]\n",
    "sitenames = [\"YELL\"]\n",
    "daily_hat = []\n",
    "\n",
    "## This iterates through all the sites in 'sitenames'\n",
    "for s in np.arange(len(sitenames)):\n",
    "    # 30min Precipitation Data\n",
    "    df_P30 = pd.read_excel(DATADIR+'PrecipData/'+str(sitenames[s])+'PrecipData.xlsx',index=False)\n",
    "    df_P30 = changeTimes.change_Pdata(df_P30)\n",
    "    \n",
    "    # Sum to Daily Precipitation Amounts\n",
    "    # checks for primary precip data\n",
    "    if 'priPrecipBulk' in df_P30:\n",
    "        df_P30b = df_P30.set_index('DateTime')\n",
    "        precip_daily = df_P30b['priPrecipBulk'].resample('D').sum() # sum total daily P\n",
    "        frac_year = df_P30b['FracYear'].resample('D').mean() # average daily FracYear (min or max better?)\n",
    "\n",
    "        daily_P = pd.DataFrame({'Total P':precip_daily,'FracYear':frac_year})\n",
    "        daily_P['Total P'].replace(0, np.nan, inplace=True)\n",
    "        daily_P['DateTime'] = daily_P.index\n",
    "\n",
    "        # Biweekly Stable Water Isotope Data\n",
    "        df_iso = pd.read_excel(DATADIR+'IsoData/'+str(sitenames[s])+'IsoData.xlsx',index=False)\n",
    "        df_iso = changeTimes.change_ISOdata(df_iso)\n",
    "\n",
    "        # Create Biweekly Precipitation Amount Timeseries to Correspond to Recorded Isotope Values\n",
    "        df_iso['setDate'] = pd.to_datetime(df_iso['setDate'])  \n",
    "        df_iso['collectDate'] = pd.to_datetime(df_iso['collectDate']) \n",
    "        \n",
    "        P14 = []\n",
    "        for i in np.arange(len(df_iso['setDate'])):\n",
    "            subset = ((df_P30['DateTime'] > df_iso['setDate'].iloc[i]) \n",
    "                    & (df_P30['DateTime'] <= df_iso['collectDate'].iloc[i]))\n",
    "\n",
    "            df_sub = df_P30.loc[subset]\n",
    "            P14.append(np.nansum(df_sub['priPrecipBulk'].values))\n",
    "\n",
    "        df_iso['Total P'] = P14\n",
    "        del P14, i, subset\n",
    "        \n",
    "        P14_2 = []; P14_3 = []; P14_4 = []\n",
    "        for i in np.arange(len(df_iso['setDate'])):\n",
    "            subset = ((daily_P['DateTime'] > df_iso['setDate'].iloc[i]) \n",
    "                    & (daily_P['DateTime'] <= df_iso['collectDate'].iloc[i]))\n",
    "\n",
    "            df_sub = daily_P.loc[subset]\n",
    "            P14_2.append(np.nansum(df_sub['Total P'].values))\n",
    "            P14_3.append(df_sub['Total P'].values.shape[0])\n",
    "            P14_4.append(np.nansum(df_sub['Total P'].values > 0))\n",
    "        \n",
    "        df_iso['Total P_2'] = P14_2\n",
    "        df_iso['Number of Days'] = P14_3\n",
    "        df_iso['Days of Precip'] = P14_4\n",
    "        del P14_2, P14_3, P14_4, i, subset\n",
    "        \n",
    "        print(df_iso)\n",
    "        stop\n",
    "    \n",
    "    # uses secondary precip data\n",
    "    else:\n",
    "        df_P30b = df_P30.set_index('DateTime')\n",
    "        precip_daily = df_P30b['secPrecipBulk'].resample('D').sum() # sum total daily P\n",
    "        frac_year = df_P30b['FracYear'].resample('D').mean() # average daily FracYear (min or max better?)\n",
    "\n",
    "        daily_P = pd.DataFrame({'Total P':precip_daily,'FracYear':frac_year})\n",
    "        daily_P['Total P'].replace(0, np.nan, inplace=True)\n",
    "        daily_P['DateTime'] = daily_P.index\n",
    "\n",
    "        # Biweekly Stable Water Isotope Data\n",
    "        df_iso = pd.read_excel(DATADIR+'IsoData/'+str(sitenames[s])+'IsoData.xlsx',index=False)\n",
    "        df_iso = changeTimes.change_ISOdata(df_iso)\n",
    "\n",
    "        # Create Biweekly Precipitation Amount Timeseries to Correspond to Recorded Isotope Values\n",
    "        df_iso['setDate'] = pd.to_datetime(df_iso['setDate'])  \n",
    "        df_iso['collectDate'] = pd.to_datetime(df_iso['collectDate']) \n",
    "        \n",
    "        P14 = []\n",
    "        for i in np.arange(len(df_iso['setDate'])):\n",
    "            subset = ((df_P30['DateTime'] > df_iso['setDate'].iloc[i]) \n",
    "                    & (df_P30['DateTime'] <= df_iso['collectDate'].iloc[i]))\n",
    "\n",
    "            df_sub = df_P30.loc[subset]\n",
    "            P14.append(np.nansum(df_sub['secPrecipBulk'].values))\n",
    "        df_iso['Total P'] = P14\n",
    "        del P14, i, subset\n",
    "        \n",
    "        P14_2 = []; P14_3 = []; P14_4 = []\n",
    "        for i in np.arange(len(df_iso['setDate'])):\n",
    "            subset = ((daily_P['DateTime'] > df_iso['setDate'].iloc[i]) \n",
    "                    & (daily_P['DateTime'] <= df_iso['collectDate'].iloc[i]))\n",
    "\n",
    "            df_sub = daily_P.loc[subset]\n",
    "            P14_2.append(np.nansum(df_sub['Total P'].values))\n",
    "            P14_3.append(df_sub['Total P'].values.shape[0])\n",
    "            P14_4.append(np.nansum(df_sub['Total P'].values > 0))\n",
    "        \n",
    "        df_iso['Total P_2'] = P14_2\n",
    "        df_iso['Number of Days'] = P14_3\n",
    "        df_iso['Days of Precip'] = P14_4\n",
    "        del P14_2, P14_3, P14_4, i, subset\n",
    "        \n",
    "        print(df_iso)\n",
    "        stop\n",
    "    \n",
    "    # Biweekly Time Series\n",
    "    df_iso = df_iso.sort_values('DateTime')\n",
    "    df_iso = df_iso.dropna(subset=['Total P'])\n",
    "    \n",
    "    # Beginning and End of time series\n",
    "    start_date_b = df_iso['DateTime'].iloc[0]\n",
    "    end_date_b = df_iso['DateTime'].iloc[-1]\n",
    "    \n",
    "    # Normalize = all days start at midnight\n",
    "    daily_series = pd.date_range(start=start_date_b, end=end_date_b, normalize=True)\n",
    "    # Remove timestamp -> index is only the date\n",
    "    df_sub = df_iso.set_index(df_iso['DateTime'].dt.date)\n",
    "    daily_series = daily_series.date\n",
    "\n",
    "    # Make DataFrame of stats between dates\n",
    "    for n in np.arange(0, len(daily_series)-14):\n",
    "        mask = (df_sub.index >= daily_series[n]) & (df_sub.index < daily_series[n+14])\n",
    "        df_temp = df_sub.loc[mask]\n",
    "        \n",
    "        if len(df_temp['Total P']) == 1:\n",
    "            # read in daily series and check how many times it rained\n",
    "            df_timeseries = pd.read_csv(OUTPUTDIR+str(sitenames[s])+'_daily_timeseries.csv')\n",
    "            df_timeseries['DateTime'] = pd.to_datetime(df_timeseries['DateTime'])\n",
    "            mask_2 = (df_timeseries['DateTime'] >= daily_series[n]) & (df_timeseries['DateTime'] < daily_series[n+14])\n",
    "            df_timeseries = df_timeseries.loc[mask_2]\n",
    "            # if it only rained once:\n",
    "            if len(df_timeseries['Total P']) ==1:\n",
    "                daily_hat.append([sitenames[s],daily_series[n], daily_series[n+14],len(df_temp['d18OWater']),\n",
    "                                  (df_temp['Total P'].values),(df_temp['d2HWater'].values),(df_temp['d18OWater'].values)])\n",
    "\n",
    "daily_hat = pd.DataFrame(daily_hat,columns=('sitename','start_date','end_date','count','P','H','O'))\n",
    "print(daily_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lindseyspencer/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [sitename, start_date, end_date, count, P, H, O]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "sitenames = [\"DELA\"]\n",
    "daily_hat = []\n",
    "\n",
    "## This iterates through all the sites in 'sitenames'\n",
    "for s in np.arange(len(sitenames)):\n",
    "    df_daily = pd.read_csv(OUTPUTDIR+str(sitenames[s])+'_daily_timeseries.csv')\n",
    "    df_daily['DateTime'] = pd.to_datetime(df_daily['DateTime'])\n",
    "    start_date_b = df_daily['DateTime'].iloc[0]\n",
    "    end_date_b = df_daily['DateTime'].iloc[-1]\n",
    "    daily_series = pd.date_range(start=start_date_b, end=end_date_b, normalize=True)\n",
    "    daily_series = daily_series.date\n",
    "    \n",
    "    # Make DataFrame of stats between dates\n",
    "    for n in np.arange(0, len(df_daily)-14):\n",
    "        mask = (df_daily['DateTime'] >= daily_series[n]) & (df_daily['DateTime'] < daily_series[n+14])\n",
    "        df_temp = df_daily.loc[mask]\n",
    "        if len(df_temp['Total P']) ==1:\n",
    "                daily_hat.append([sitenames[s],df_daily['DateTime'].iloc[n], df_daily['DateTime'].iloc[n+14],\n",
    "                                  len(df_temp['d18O_1']),(df_temp['Total P'].values),\n",
    "                                  (df_temp['d2H_1'].values),(df_temp['d18O_1'].values)])\n",
    "        \n",
    "daily_hat = pd.DataFrame(daily_hat,columns=('sitename','start_date','end_date','count','P','H','O'))\n",
    "print(daily_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
